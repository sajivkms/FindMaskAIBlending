{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqiEXqoduTS6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNo-JfkhuUej"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from model.mask_rcnn import maskrcnn_resnet50_fpn, MaskRCNNPredictor\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RrmANxAzD7S"
   },
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "        \"__background__ \",\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUB6BCqo1gs3"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7oUklJey_S5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZtTHVgGxEzi"
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    # transforms.append(PrepareItem())\n",
    "    \n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf4c-q2A_kTo"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import download_url, check_integrity, verify_str_arg\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_are6VqACE-"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1sZTQ6j_vHf"
   },
   "outputs": [],
   "source": [
    "DATASET_YEAR_DICT = {\n",
    "    '2012': {\n",
    "        'url': 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar',\n",
    "        'filename': 'VOCtrainval_11-May-2012.tar',\n",
    "        'md5': '6cd6e144f989b92b3379bac3b3de84fd',\n",
    "        'base_dir': os.path.join('VOCdevkit', 'VOC2012')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNQQWObn-1Vb"
   },
   "outputs": [],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar \n",
    "!mkdir VOC2012\n",
    "!tar xvf /content/VOCtrainval_11-May-2012.tar -C /content/VOC2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUd-hebk_ZNc"
   },
   "outputs": [],
   "source": [
    "class VOCSegmentation(VisionDataset):\n",
    "    \"\"\"`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Segmentation Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of the VOC Dataset.\n",
    "        year (string, optional): The dataset year, supports years 2007 to 2012.\n",
    "        image_set (string, optional): Select the image_set to use, ``train``, ``trainval`` or ``val``\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 year='2012',\n",
    "                 image_set='train',\n",
    "                 download=False,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 transforms=None):\n",
    "        super(VOCSegmentation, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.year = year\n",
    "        self.url = DATASET_YEAR_DICT[year]['url']\n",
    "        self.filename = DATASET_YEAR_DICT[year]['filename']\n",
    "        self.md5 = DATASET_YEAR_DICT[year]['md5']\n",
    "        valid_sets = [\"train\", \"trainval\", \"val\"]\n",
    "        if year == \"2007\":\n",
    "            valid_sets.append(\"test\")\n",
    "        self.image_set = verify_str_arg(image_set, \"image_set\", valid_sets)\n",
    "        base_dir = DATASET_YEAR_DICT[year]['base_dir']\n",
    "        voc_root = os.path.join(self.root, base_dir)\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        mask_dir = os.path.join(voc_root, 'SegmentationObject')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if download:\n",
    "            download_extract(self.url, self.root, self.filename, self.md5)\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Segmentation')\n",
    "\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.masks = [os.path.join(mask_dir, x + \".png\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.masks))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is the image segmentation.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        mask = Image.open(self.masks[index])\n",
    "        mask = np.array(mask)\n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "        tgt = self.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot()\n",
    "        )\n",
    "        bboxes = tgt['annotation']['object']\n",
    "        if len(obj_ids) > 1:\n",
    "          if type(bboxes) == list and len(bboxes) != len(obj_ids):\n",
    "            obj_ids = obj_ids[:-1]\n",
    "          if type(bboxes) == dict:\n",
    "            obj_ids = obj_ids[:-1]\n",
    "        num_objs = len(obj_ids)\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        # plt.imshow(mask)\n",
    "        if num_objs == 1:\n",
    "          boxes.append([int(bboxes['bndbox']['xmin']),\n",
    "                          int(bboxes['bndbox']['ymin']),\n",
    "                          int(bboxes['bndbox']['xmax']),\n",
    "                          int(bboxes['bndbox']['ymax'])])\n",
    "          labels.append(CLASSES.index(bboxes['name']))\n",
    "        else:\n",
    "          for i in range(num_objs):\n",
    "            boxes.append([int(bboxes[i]['bndbox']['xmin']),\n",
    "                          int(bboxes[i]['bndbox']['ymin']),\n",
    "                          int(bboxes[i]['bndbox']['xmax']),\n",
    "                          int(bboxes[i]['bndbox']['ymax'])])\n",
    "            labels.append(CLASSES.index(bboxes[i]['name']))\n",
    "          \n",
    "        image_id = torch.tensor([index])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_voc_xml(self, node):\n",
    "        voc_dict = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic = collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_roV9dwtu5iF"
   },
   "outputs": [],
   "source": [
    "dataset = VOCSegmentation(\"VOC2012\", \n",
    "                                            year='2012', \n",
    "                                            transforms=get_transform(train=True),\n",
    "                                            download=False)\n",
    "dataset_test = VOCSegmentation(\"VOC2012\", \n",
    "                                            year='2012', image_set='val', \n",
    "                                            transforms=get_transform(train=False),\n",
    "                                            download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "b7kD3UeKHc_x",
    "outputId": "6f2698db-8347-4d29-f9ed-18451c540e61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8392, 0.9294, 1.0000,  ..., 1.0000, 1.0000, 0.6784],\n",
       "          [0.9686, 1.0000, 1.0000,  ..., 0.9843, 1.0000, 0.4863],\n",
       "          [0.9961, 0.9725, 0.9725,  ..., 0.9725, 1.0000, 0.2196],\n",
       "          ...,\n",
       "          [0.3294, 0.3529, 0.3686,  ..., 0.4118, 0.2471, 0.2510],\n",
       "          [0.3216, 0.2118, 0.3216,  ..., 0.2157, 0.2706, 0.2667],\n",
       "          [0.2118, 0.3490, 0.2471,  ..., 0.3137, 0.1686, 0.2627]],\n",
       " \n",
       "         [[0.8314, 0.9216, 1.0000,  ..., 0.9608, 0.9647, 0.6000],\n",
       "          [0.9569, 0.9961, 0.9882,  ..., 0.9569, 0.9765, 0.4118],\n",
       "          [0.9922, 0.9686, 0.9686,  ..., 0.9725, 0.9922, 0.1451],\n",
       "          ...,\n",
       "          [0.3569, 0.3882, 0.4118,  ..., 0.4431, 0.2902, 0.3059],\n",
       "          [0.3765, 0.2745, 0.3843,  ..., 0.2471, 0.3216, 0.3373],\n",
       "          [0.2549, 0.3961, 0.2941,  ..., 0.3765, 0.2353, 0.3412]],\n",
       " \n",
       "         [[0.7843, 0.8745, 0.9569,  ..., 0.9608, 0.9490, 0.5569],\n",
       "          [0.9294, 0.9686, 0.9608,  ..., 0.9255, 0.9333, 0.3529],\n",
       "          [0.9765, 0.9529, 0.9529,  ..., 0.9255, 0.9373, 0.0784],\n",
       "          ...,\n",
       "          [0.3804, 0.4078, 0.4275,  ..., 0.4510, 0.3137, 0.3412],\n",
       "          [0.3804, 0.2745, 0.3843,  ..., 0.2588, 0.3451, 0.3765],\n",
       "          [0.2627, 0.3961, 0.2941,  ..., 0.3765, 0.2627, 0.3765]]]),\n",
       " {'area': tensor([ 40960., 132022.]),\n",
       "  'boxes': tensor([[121., 115., 377., 275.],\n",
       "          [ 72.,   1., 425., 375.]]),\n",
       "  'image_id': tensor([2]),\n",
       "  'iscrowd': tensor([0, 0]),\n",
       "  'labels': tensor([12,  9]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "mVMPJVYTFwsM",
    "outputId": "5edd460d-c753-4e3a-e2df-9627b5fed186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "#1012, 1382 814\n",
    "for i in range(len(dataset_test)):\n",
    "  print(\".\", end=\"\")\n",
    "  try:\n",
    "    temp = dataset[i]\n",
    "  except Exception as e:\n",
    "    print(i)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiunUyXNxWH8"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HS3mVsX5xsD1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JgBfqRExpB6"
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O0tfdXj-EySq",
    "outputId": "130345d8-adf1-447a-dc69-bd4bbf8dd01f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbnOJ3YJE7Ar"
   },
   "outputs": [],
   "source": [
    "######### From the TorchVision Tutorial ##########\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
    "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = self.delimiter.join([\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}',\n",
    "            'max mem: {memory:.0f}'\n",
    "        ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                print(log_msg.format(\n",
    "                    i, len(iterable), eta=eta_string,\n",
    "                    meters=str(self),\n",
    "                    time=str(iter_time), data=str(data_time),\n",
    "                    memory=torch.cuda.max_memory_allocated() / MB))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}'.format(\n",
    "        args.rank, args.dist_url), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "e5854b3be22d4cb78942f5857f213fba",
      "3a9ce00a0af945fa928528ff6a756da1",
      "9961355ee66a4f54ac3bee57b283f2c9",
      "54072b0d377549cdb8ca8f8b9f65e53f",
      "15b31a8246a04b51ba2465ff89da9d03",
      "26d7a97d207f47479347f7963c722a70",
      "cced02ba3c834081b73a43e188ac73b1",
      "2a61e2369d9d428e89bd98cb82a31a06"
     ]
    },
    "colab_type": "code",
    "id": "yglNN4DEEz5C",
    "outputId": "fe5a807d-5821-42b6-8df5-5c9d8efdb30b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5854b3be22d4cb78942f5857f213fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=178090079), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has 21 classes, 20 items and 1 background class\n",
    "num_classes = len(CLASSES)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the either CPU or GPU - GPU is recommended\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad] # The optimizer requires the parameters of the model\n",
    "# to be passed in.\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, # Using Stochastic Gradient Descent as our optimizer\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP9170ayE2X6"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbC0AFYSFI-S"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train() # Setting the model to training mode.\n",
    "    metric_logger = MetricLogger(delimiter=\"  \") # This is a utility provided by the Torchvision tutorial\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}')) # We are taking advantage of it.\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "    \n",
    "    ## Typical Training loop ##\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header=header):\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        ## Getting the outputs\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        ## Resetting the Gradients ##\n",
    "        optimizer.zero_grad()\n",
    "        ## Backpropagation ##\n",
    "        losses.backward()\n",
    "        ## Updating the weights ##\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        # For logging\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "id": "KvdEQnh4FJ9h",
    "outputId": "6c0a0d6e-fec0-40ff-bbc9-6bbae436f45f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/732]  eta: 0:20:52  lr: 0.000012  loss: 4.2840 (4.2840)  loss_classifier: 2.9888 (2.9888)  loss_box_reg: 0.1629 (0.1629)  loss_mask: 1.0780 (1.0780)  loss_objectness: 0.0063 (0.0063)  loss_rpn_box_reg: 0.0480 (0.0480)  time: 1.7107  data: 0.2422  max mem: 3115\n",
      "Epoch: [0]  [ 10/732]  eta: 0:06:07  lr: 0.000080  loss: 4.5183 (4.6226)  loss_classifier: 2.9734 (2.9412)  loss_box_reg: 0.2211 (0.2164)  loss_mask: 1.2351 (1.4203)  loss_objectness: 0.0118 (0.0135)  loss_rpn_box_reg: 0.0164 (0.0312)  time: 0.5087  data: 0.0281  max mem: 3410\n",
      "Epoch: [0]  [ 20/732]  eta: 0:05:25  lr: 0.000148  loss: 3.9940 (3.8943)  loss_classifier: 2.3729 (2.3270)  loss_box_reg: 0.1860 (0.1902)  loss_mask: 1.2351 (1.3409)  loss_objectness: 0.0092 (0.0119)  loss_rpn_box_reg: 0.0124 (0.0244)  time: 0.3948  data: 0.0065  max mem: 3726\n",
      "Epoch: [0]  [ 30/732]  eta: 0:05:08  lr: 0.000217  loss: 2.2239 (3.2726)  loss_classifier: 0.8993 (1.7731)  loss_box_reg: 0.1913 (0.2137)  loss_mask: 0.9946 (1.2519)  loss_objectness: 0.0061 (0.0113)  loss_rpn_box_reg: 0.0124 (0.0226)  time: 0.4010  data: 0.0064  max mem: 3726\n",
      "Epoch: [0]  [ 40/732]  eta: 0:04:53  lr: 0.000285  loss: 1.8709 (2.8488)  loss_classifier: 0.5010 (1.4450)  loss_box_reg: 0.2422 (0.2156)  loss_mask: 0.8399 (1.1567)  loss_objectness: 0.0061 (0.0112)  loss_rpn_box_reg: 0.0126 (0.0204)  time: 0.3906  data: 0.0064  max mem: 3726\n",
      "Epoch: [0]  [ 50/732]  eta: 0:04:45  lr: 0.000353  loss: 1.3432 (2.5719)  loss_classifier: 0.3792 (1.2506)  loss_box_reg: 0.2069 (0.2212)  loss_mask: 0.7400 (1.0684)  loss_objectness: 0.0069 (0.0113)  loss_rpn_box_reg: 0.0126 (0.0203)  time: 0.3852  data: 0.0065  max mem: 3726\n",
      "Epoch: [0]  [ 60/732]  eta: 0:04:37  lr: 0.000422  loss: 1.1638 (2.3347)  loss_classifier: 0.3278 (1.0898)  loss_box_reg: 0.1803 (0.2102)  loss_mask: 0.6824 (1.0036)  loss_objectness: 0.0062 (0.0106)  loss_rpn_box_reg: 0.0175 (0.0204)  time: 0.3874  data: 0.0070  max mem: 3726\n",
      "Epoch: [0]  [ 70/732]  eta: 0:04:30  lr: 0.000490  loss: 1.1334 (2.1866)  loss_classifier: 0.2807 (0.9858)  loss_box_reg: 0.1383 (0.2077)  loss_mask: 0.6639 (0.9607)  loss_objectness: 0.0040 (0.0113)  loss_rpn_box_reg: 0.0187 (0.0211)  time: 0.3836  data: 0.0073  max mem: 3726\n",
      "Epoch: [0]  [ 80/732]  eta: 0:04:24  lr: 0.000558  loss: 1.1845 (2.0794)  loss_classifier: 0.2807 (0.9075)  loss_box_reg: 0.1536 (0.2049)  loss_mask: 0.6975 (0.9309)  loss_objectness: 0.0060 (0.0122)  loss_rpn_box_reg: 0.0229 (0.0238)  time: 0.3847  data: 0.0073  max mem: 3726\n",
      "Epoch: [0]  [ 90/732]  eta: 0:04:20  lr: 0.000627  loss: 1.0473 (1.9789)  loss_classifier: 0.2185 (0.8431)  loss_box_reg: 0.1194 (0.2017)  loss_mask: 0.6575 (0.8989)  loss_objectness: 0.0062 (0.0117)  loss_rpn_box_reg: 0.0211 (0.0236)  time: 0.3976  data: 0.0070  max mem: 3726\n",
      "Epoch: [0]  [100/732]  eta: 0:04:16  lr: 0.000695  loss: 1.0473 (1.8957)  loss_classifier: 0.2517 (0.7914)  loss_box_reg: 0.1435 (0.2009)  loss_mask: 0.6136 (0.8683)  loss_objectness: 0.0059 (0.0112)  loss_rpn_box_reg: 0.0162 (0.0238)  time: 0.4044  data: 0.0066  max mem: 3726\n",
      "Epoch: [0]  [110/732]  eta: 0:04:11  lr: 0.000763  loss: 1.1304 (1.8319)  loss_classifier: 0.3203 (0.7527)  loss_box_reg: 0.2167 (0.2023)  loss_mask: 0.5829 (0.8422)  loss_objectness: 0.0087 (0.0115)  loss_rpn_box_reg: 0.0150 (0.0233)  time: 0.3930  data: 0.0065  max mem: 3726\n",
      "Epoch: [0]  [120/732]  eta: 0:04:06  lr: 0.000832  loss: 1.0979 (1.7662)  loss_classifier: 0.2719 (0.7146)  loss_box_reg: 0.1858 (0.1996)  loss_mask: 0.5503 (0.8170)  loss_objectness: 0.0076 (0.0116)  loss_rpn_box_reg: 0.0149 (0.0233)  time: 0.3866  data: 0.0065  max mem: 4060\n",
      "Epoch: [0]  [130/732]  eta: 0:04:01  lr: 0.000900  loss: 0.8399 (1.7039)  loss_classifier: 0.2085 (0.6794)  loss_box_reg: 0.1142 (0.1961)  loss_mask: 0.5288 (0.7944)  loss_objectness: 0.0067 (0.0113)  loss_rpn_box_reg: 0.0137 (0.0227)  time: 0.3894  data: 0.0065  max mem: 4060\n",
      "Epoch: [0]  [140/732]  eta: 0:03:57  lr: 0.000968  loss: 0.8566 (1.6554)  loss_classifier: 0.2298 (0.6526)  loss_box_reg: 0.1378 (0.1954)  loss_mask: 0.4896 (0.7737)  loss_objectness: 0.0085 (0.0111)  loss_rpn_box_reg: 0.0101 (0.0226)  time: 0.3978  data: 0.0067  max mem: 4063\n",
      "Epoch: [0]  [150/732]  eta: 0:03:53  lr: 0.001037  loss: 0.9170 (1.6061)  loss_classifier: 0.2195 (0.6236)  loss_box_reg: 0.1325 (0.1915)  loss_mask: 0.5078 (0.7568)  loss_objectness: 0.0034 (0.0106)  loss_rpn_box_reg: 0.0191 (0.0235)  time: 0.4038  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [160/732]  eta: 0:03:49  lr: 0.001105  loss: 0.8632 (1.5662)  loss_classifier: 0.2141 (0.6031)  loss_box_reg: 0.1324 (0.1917)  loss_mask: 0.4589 (0.7375)  loss_objectness: 0.0034 (0.0103)  loss_rpn_box_reg: 0.0247 (0.0236)  time: 0.4014  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [170/732]  eta: 0:03:46  lr: 0.001173  loss: 0.8481 (1.5269)  loss_classifier: 0.2273 (0.5831)  loss_box_reg: 0.1430 (0.1908)  loss_mask: 0.4318 (0.7192)  loss_objectness: 0.0017 (0.0102)  loss_rpn_box_reg: 0.0094 (0.0236)  time: 0.4100  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [180/732]  eta: 0:03:42  lr: 0.001242  loss: 0.7262 (1.4882)  loss_classifier: 0.2112 (0.5645)  loss_box_reg: 0.1430 (0.1906)  loss_mask: 0.3927 (0.6998)  loss_objectness: 0.0017 (0.0101)  loss_rpn_box_reg: 0.0096 (0.0233)  time: 0.4090  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [190/732]  eta: 0:03:38  lr: 0.001310  loss: 0.7841 (1.4589)  loss_classifier: 0.2102 (0.5500)  loss_box_reg: 0.1627 (0.1897)  loss_mask: 0.3746 (0.6856)  loss_objectness: 0.0042 (0.0101)  loss_rpn_box_reg: 0.0216 (0.0235)  time: 0.3984  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [200/732]  eta: 0:03:34  lr: 0.001378  loss: 0.8604 (1.4322)  loss_classifier: 0.2102 (0.5378)  loss_box_reg: 0.1627 (0.1892)  loss_mask: 0.4072 (0.6713)  loss_objectness: 0.0069 (0.0107)  loss_rpn_box_reg: 0.0185 (0.0232)  time: 0.4056  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [210/732]  eta: 0:03:29  lr: 0.001447  loss: 0.7798 (1.4115)  loss_classifier: 0.2677 (0.5289)  loss_box_reg: 0.1389 (0.1901)  loss_mask: 0.4072 (0.6586)  loss_objectness: 0.0093 (0.0110)  loss_rpn_box_reg: 0.0145 (0.0230)  time: 0.4021  data: 0.0067  max mem: 4063\n",
      "Epoch: [0]  [220/732]  eta: 0:03:25  lr: 0.001515  loss: 0.7801 (1.3914)  loss_classifier: 0.2677 (0.5190)  loss_box_reg: 0.1411 (0.1911)  loss_mask: 0.3749 (0.6465)  loss_objectness: 0.0082 (0.0118)  loss_rpn_box_reg: 0.0145 (0.0231)  time: 0.3903  data: 0.0067  max mem: 4063\n",
      "Epoch: [0]  [230/732]  eta: 0:03:21  lr: 0.001583  loss: 0.8259 (1.3682)  loss_classifier: 0.2267 (0.5107)  loss_box_reg: 0.1601 (0.1920)  loss_mask: 0.3179 (0.6306)  loss_objectness: 0.0070 (0.0117)  loss_rpn_box_reg: 0.0160 (0.0232)  time: 0.4008  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [240/732]  eta: 0:03:17  lr: 0.001652  loss: 0.6684 (1.3402)  loss_classifier: 0.2175 (0.4981)  loss_box_reg: 0.1465 (0.1900)  loss_mask: 0.2858 (0.6169)  loss_objectness: 0.0044 (0.0123)  loss_rpn_box_reg: 0.0115 (0.0229)  time: 0.4000  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [250/732]  eta: 0:03:12  lr: 0.001720  loss: 0.6191 (1.3131)  loss_classifier: 0.1794 (0.4855)  loss_box_reg: 0.1130 (0.1875)  loss_mask: 0.2858 (0.6049)  loss_objectness: 0.0053 (0.0122)  loss_rpn_box_reg: 0.0130 (0.0231)  time: 0.3798  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [260/732]  eta: 0:03:09  lr: 0.001788  loss: 0.5883 (1.2885)  loss_classifier: 0.1455 (0.4749)  loss_box_reg: 0.1191 (0.1858)  loss_mask: 0.2893 (0.5925)  loss_objectness: 0.0060 (0.0123)  loss_rpn_box_reg: 0.0176 (0.0231)  time: 0.3935  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [270/732]  eta: 0:03:04  lr: 0.001857  loss: 0.6216 (1.2716)  loss_classifier: 0.1733 (0.4659)  loss_box_reg: 0.1193 (0.1846)  loss_mask: 0.3012 (0.5847)  loss_objectness: 0.0074 (0.0131)  loss_rpn_box_reg: 0.0193 (0.0232)  time: 0.3905  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [280/732]  eta: 0:03:00  lr: 0.001925  loss: 0.7563 (1.2557)  loss_classifier: 0.1961 (0.4569)  loss_box_reg: 0.1237 (0.1836)  loss_mask: 0.3588 (0.5786)  loss_objectness: 0.0094 (0.0131)  loss_rpn_box_reg: 0.0216 (0.0235)  time: 0.3834  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [290/732]  eta: 0:02:56  lr: 0.001993  loss: 0.7199 (1.2385)  loss_classifier: 0.1965 (0.4491)  loss_box_reg: 0.1549 (0.1836)  loss_mask: 0.3218 (0.5695)  loss_objectness: 0.0094 (0.0129)  loss_rpn_box_reg: 0.0125 (0.0234)  time: 0.4039  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [300/732]  eta: 0:02:52  lr: 0.002062  loss: 0.7867 (1.2220)  loss_classifier: 0.2005 (0.4409)  loss_box_reg: 0.1666 (0.1835)  loss_mask: 0.3000 (0.5611)  loss_objectness: 0.0074 (0.0131)  loss_rpn_box_reg: 0.0119 (0.0235)  time: 0.4077  data: 0.0069  max mem: 4063\n",
      "Epoch: [0]  [310/732]  eta: 0:02:48  lr: 0.002130  loss: 0.6858 (1.2024)  loss_classifier: 0.1817 (0.4323)  loss_box_reg: 0.1491 (0.1817)  loss_mask: 0.2634 (0.5518)  loss_objectness: 0.0111 (0.0135)  loss_rpn_box_reg: 0.0126 (0.0232)  time: 0.4006  data: 0.0070  max mem: 4063\n",
      "Epoch: [0]  [320/732]  eta: 0:02:44  lr: 0.002198  loss: 0.6838 (1.1872)  loss_classifier: 0.1424 (0.4241)  loss_box_reg: 0.1491 (0.1814)  loss_mask: 0.2935 (0.5451)  loss_objectness: 0.0116 (0.0134)  loss_rpn_box_reg: 0.0175 (0.0232)  time: 0.3915  data: 0.0067  max mem: 4063\n",
      "Epoch: [0]  [330/732]  eta: 0:02:40  lr: 0.002267  loss: 0.6321 (1.1712)  loss_classifier: 0.1419 (0.4152)  loss_box_reg: 0.1565 (0.1794)  loss_mask: 0.3006 (0.5401)  loss_objectness: 0.0058 (0.0133)  loss_rpn_box_reg: 0.0211 (0.0232)  time: 0.4076  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [340/732]  eta: 0:02:36  lr: 0.002335  loss: 0.6570 (1.1584)  loss_classifier: 0.1673 (0.4089)  loss_box_reg: 0.1331 (0.1792)  loss_mask: 0.3120 (0.5341)  loss_objectness: 0.0058 (0.0132)  loss_rpn_box_reg: 0.0139 (0.0230)  time: 0.4041  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [350/732]  eta: 0:02:32  lr: 0.002403  loss: 0.6692 (1.1429)  loss_classifier: 0.1850 (0.4016)  loss_box_reg: 0.1387 (0.1779)  loss_mask: 0.3108 (0.5275)  loss_objectness: 0.0085 (0.0130)  loss_rpn_box_reg: 0.0139 (0.0230)  time: 0.3942  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [360/732]  eta: 0:02:28  lr: 0.002472  loss: 0.6260 (1.1298)  loss_classifier: 0.1331 (0.3954)  loss_box_reg: 0.1136 (0.1767)  loss_mask: 0.2897 (0.5214)  loss_objectness: 0.0065 (0.0131)  loss_rpn_box_reg: 0.0143 (0.0230)  time: 0.3901  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [370/732]  eta: 0:02:24  lr: 0.002540  loss: 0.5447 (1.1185)  loss_classifier: 0.1161 (0.3893)  loss_box_reg: 0.1062 (0.1764)  loss_mask: 0.3127 (0.5150)  loss_objectness: 0.0047 (0.0144)  loss_rpn_box_reg: 0.0170 (0.0234)  time: 0.3877  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [380/732]  eta: 0:02:20  lr: 0.002608  loss: 0.5637 (1.1066)  loss_classifier: 0.1290 (0.3832)  loss_box_reg: 0.1086 (0.1752)  loss_mask: 0.3127 (0.5099)  loss_objectness: 0.0081 (0.0146)  loss_rpn_box_reg: 0.0235 (0.0237)  time: 0.3895  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [390/732]  eta: 0:02:16  lr: 0.002677  loss: 0.6130 (1.0920)  loss_classifier: 0.1327 (0.3767)  loss_box_reg: 0.1173 (0.1738)  loss_mask: 0.2842 (0.5029)  loss_objectness: 0.0133 (0.0147)  loss_rpn_box_reg: 0.0209 (0.0238)  time: 0.3798  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [400/732]  eta: 0:02:12  lr: 0.002745  loss: 0.5197 (1.0817)  loss_classifier: 0.1149 (0.3714)  loss_box_reg: 0.0963 (0.1722)  loss_mask: 0.2705 (0.4993)  loss_objectness: 0.0123 (0.0148)  loss_rpn_box_reg: 0.0163 (0.0240)  time: 0.3909  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [410/732]  eta: 0:02:08  lr: 0.002813  loss: 0.5246 (1.0702)  loss_classifier: 0.1329 (0.3665)  loss_box_reg: 0.0934 (0.1706)  loss_mask: 0.2705 (0.4940)  loss_objectness: 0.0113 (0.0150)  loss_rpn_box_reg: 0.0200 (0.0241)  time: 0.3843  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [420/732]  eta: 0:02:04  lr: 0.002882  loss: 0.5571 (1.0572)  loss_classifier: 0.1149 (0.3607)  loss_box_reg: 0.0840 (0.1686)  loss_mask: 0.2634 (0.4893)  loss_objectness: 0.0047 (0.0147)  loss_rpn_box_reg: 0.0163 (0.0239)  time: 0.3805  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [430/732]  eta: 0:02:00  lr: 0.002950  loss: 0.5622 (1.0469)  loss_classifier: 0.1273 (0.3555)  loss_box_reg: 0.0809 (0.1671)  loss_mask: 0.3031 (0.4857)  loss_objectness: 0.0027 (0.0145)  loss_rpn_box_reg: 0.0125 (0.0240)  time: 0.4025  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [440/732]  eta: 0:01:56  lr: 0.003018  loss: 0.5671 (1.0370)  loss_classifier: 0.1372 (0.3505)  loss_box_reg: 0.1086 (0.1661)  loss_mask: 0.3031 (0.4820)  loss_objectness: 0.0060 (0.0146)  loss_rpn_box_reg: 0.0199 (0.0239)  time: 0.4214  data: 0.0068  max mem: 4063\n",
      "Epoch: [0]  [450/732]  eta: 0:01:52  lr: 0.003087  loss: 0.6175 (1.0284)  loss_classifier: 0.1328 (0.3461)  loss_box_reg: 0.1255 (0.1648)  loss_mask: 0.2785 (0.4785)  loss_objectness: 0.0133 (0.0150)  loss_rpn_box_reg: 0.0220 (0.0239)  time: 0.4135  data: 0.0067  max mem: 4063\n",
      "Epoch: [0]  [460/732]  eta: 0:01:48  lr: 0.003155  loss: 0.4775 (1.0186)  loss_classifier: 0.1292 (0.3418)  loss_box_reg: 0.0971 (0.1633)  loss_mask: 0.2444 (0.4741)  loss_objectness: 0.0136 (0.0152)  loss_rpn_box_reg: 0.0227 (0.0242)  time: 0.3968  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [470/732]  eta: 0:01:44  lr: 0.003223  loss: 0.4775 (1.0079)  loss_classifier: 0.1229 (0.3372)  loss_box_reg: 0.0880 (0.1618)  loss_mask: 0.2384 (0.4696)  loss_objectness: 0.0118 (0.0151)  loss_rpn_box_reg: 0.0196 (0.0242)  time: 0.3923  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [480/732]  eta: 0:01:40  lr: 0.003292  loss: 0.5216 (0.9995)  loss_classifier: 0.1116 (0.3326)  loss_box_reg: 0.0880 (0.1606)  loss_mask: 0.2699 (0.4671)  loss_objectness: 0.0094 (0.0151)  loss_rpn_box_reg: 0.0179 (0.0241)  time: 0.3882  data: 0.0064  max mem: 4063\n",
      "Epoch: [0]  [490/732]  eta: 0:01:36  lr: 0.003360  loss: 0.5671 (0.9914)  loss_classifier: 0.0971 (0.3287)  loss_box_reg: 0.1024 (0.1592)  loss_mask: 0.3240 (0.4646)  loss_objectness: 0.0046 (0.0149)  loss_rpn_box_reg: 0.0175 (0.0240)  time: 0.4022  data: 0.0065  max mem: 4063\n",
      "Epoch: [0]  [500/732]  eta: 0:01:32  lr: 0.003428  loss: 0.6638 (0.9850)  loss_classifier: 0.1473 (0.3261)  loss_box_reg: 0.1018 (0.1581)  loss_mask: 0.3202 (0.4622)  loss_objectness: 0.0034 (0.0147)  loss_rpn_box_reg: 0.0147 (0.0239)  time: 0.4118  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [510/732]  eta: 0:01:28  lr: 0.003497  loss: 0.6057 (0.9776)  loss_classifier: 0.1362 (0.3224)  loss_box_reg: 0.1017 (0.1570)  loss_mask: 0.2892 (0.4595)  loss_objectness: 0.0069 (0.0146)  loss_rpn_box_reg: 0.0147 (0.0240)  time: 0.4134  data: 0.0066  max mem: 4063\n",
      "Epoch: [0]  [520/732]  eta: 0:01:24  lr: 0.003565  loss: 0.5584 (0.9697)  loss_classifier: 0.1068 (0.3184)  loss_box_reg: 0.0710 (0.1560)  loss_mask: 0.2716 (0.4569)  loss_objectness: 0.0059 (0.0145)  loss_rpn_box_reg: 0.0164 (0.0239)  time: 0.3975  data: 0.0066  max mem: 4063\n"
     ]
    }
   ],
   "source": [
    "# Training for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX5obSmIFmSl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15b31a8246a04b51ba2465ff89da9d03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "26d7a97d207f47479347f7963c722a70": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a61e2369d9d428e89bd98cb82a31a06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a9ce00a0af945fa928528ff6a756da1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54072b0d377549cdb8ca8f8b9f65e53f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a61e2369d9d428e89bd98cb82a31a06",
      "placeholder": "​",
      "style": "IPY_MODEL_cced02ba3c834081b73a43e188ac73b1",
      "value": " 170M/170M [00:05&lt;00:00, 33.0MB/s]"
     }
    },
    "9961355ee66a4f54ac3bee57b283f2c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26d7a97d207f47479347f7963c722a70",
      "max": 178090079,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_15b31a8246a04b51ba2465ff89da9d03",
      "value": 178090079
     }
    },
    "cced02ba3c834081b73a43e188ac73b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5854b3be22d4cb78942f5857f213fba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9961355ee66a4f54ac3bee57b283f2c9",
       "IPY_MODEL_54072b0d377549cdb8ca8f8b9f65e53f"
      ],
      "layout": "IPY_MODEL_3a9ce00a0af945fa928528ff6a756da1"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
